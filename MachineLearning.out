\BOOKMARK [1][-]{section.1}{Training Models/Algorithms}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Linear Regression}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Gradient Descent}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Polynomial Regression}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Regularized Linear Models}{section.1}% 5
\BOOKMARK [3][-]{subsubsection.1.4.1}{Ridge Regression}{subsection.1.4}% 6
\BOOKMARK [3][-]{subsubsection.1.4.2}{LASSO}{subsection.1.4}% 7
\BOOKMARK [3][-]{subsubsection.1.4.3}{Elastic Net}{subsection.1.4}% 8
\BOOKMARK [3][-]{subsubsection.1.4.4}{When to use Linear Regression, LASSO, Ridge, or Elastic Net?}{subsection.1.4}% 9
\BOOKMARK [2][-]{subsection.1.5}{Logistic Regression}{section.1}% 10
\BOOKMARK [3][-]{subsubsection.1.5.1}{Estimating Probabilities}{subsection.1.5}% 11
\BOOKMARK [3][-]{subsubsection.1.5.2}{Training and Cost Function}{subsection.1.5}% 12
\BOOKMARK [1][-]{section.2}{Classification}{}% 13
\BOOKMARK [2][-]{subsection.2.1}{MNIST}{section.2}% 14
\BOOKMARK [2][-]{subsection.2.2}{Training a Binary Classifier}{section.2}% 15
\BOOKMARK [2][-]{subsection.2.3}{Performance Measures}{section.2}% 16
\BOOKMARK [3][-]{subsubsection.2.3.1}{Measuring Accuracy Using Cross-Validation}{subsection.2.3}% 17
\BOOKMARK [3][-]{subsubsection.2.3.2}{Confusion Matrix}{subsection.2.3}% 18
\BOOKMARK [3][-]{subsubsection.2.3.3}{Precision and Recall}{subsection.2.3}% 19
\BOOKMARK [3][-]{subsubsection.2.3.4}{ROC Curve}{subsection.2.3}% 20
\BOOKMARK [2][-]{subsection.2.4}{Multiclass Classifification}{section.2}% 21
\BOOKMARK [1][-]{section.3}{Support Vector Machines}{}% 22
\BOOKMARK [2][-]{subsection.3.1}{How SVMs Work}{section.3}% 23
\BOOKMARK [2][-]{subsection.3.2}{Linear SVM Classification}{section.3}% 24
\BOOKMARK [3][-]{subsubsection.3.2.1}{Hard Margin Classification}{subsection.3.2}% 25
\BOOKMARK [3][-]{subsubsection.3.2.2}{Soft Margin Classification}{subsection.3.2}% 26
\BOOKMARK [2][-]{subsection.3.3}{Non-Linear SVM Classification}{section.3}% 27
\BOOKMARK [3][-]{subsubsection.3.3.1}{Polynomial Kernel}{subsection.3.3}% 28
\BOOKMARK [3][-]{subsubsection.3.3.2}{Gaussian RBF Kernel}{subsection.3.3}% 29
\BOOKMARK [1][-]{section.4}{Random Forest/Ensemble Learning}{}% 30
\BOOKMARK [2][-]{subsection.4.1}{Decision Trees}{section.4}% 31
\BOOKMARK [2][-]{subsection.4.2}{Random Forests}{section.4}% 32
\BOOKMARK [2][-]{subsection.4.3}{Voting Classifiers}{section.4}% 33
\BOOKMARK [2][-]{subsection.4.4}{Bagging and Pasting}{section.4}% 34
\BOOKMARK [2][-]{subsection.4.5}{Boosting}{section.4}% 35
\BOOKMARK [2][-]{subsection.4.6}{Stacking}{section.4}% 36
\BOOKMARK [1][-]{section.5}{Dimensionality Reduction}{}% 37
\BOOKMARK [2][-]{subsection.5.1}{Curse of Dimensionality}{section.5}% 38
\BOOKMARK [2][-]{subsection.5.2}{Main Approaches for Dimensionality Reduction}{section.5}% 39
\BOOKMARK [3][-]{subsubsection.5.2.1}{Projection}{subsection.5.2}% 40
\BOOKMARK [3][-]{subsubsection.5.2.2}{Manifold Learning}{subsection.5.2}% 41
\BOOKMARK [2][-]{subsection.5.3}{Principal Compenents Analysis \(PCA\)}{section.5}% 42
\BOOKMARK [2][-]{subsection.5.4}{Kernel PCA}{section.5}% 43
\BOOKMARK [2][-]{subsection.5.5}{Locally Linear Embedding \(LLE\)}{section.5}% 44
\BOOKMARK [2][-]{subsection.5.6}{Other Dimensionality Reduction Techniques}{section.5}% 45
\BOOKMARK [1][-]{section.6}{Unsupervised Learning}{}% 46
\BOOKMARK [2][-]{subsection.6.1}{Clustering}{section.6}% 47
\BOOKMARK [2][-]{subsection.6.2}{Gaussian Mixtures}{section.6}% 48
